{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This code is taken from https://github.com/PaulLyonel/conditionalSNF/blob/9e365bda3507c8249eecd0294a73bc8e88ac7809/utils/Util_mixture.py#L64\n",
    "and serves as a gentle Tutorial into INN'S"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3496679859.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Input \u001B[0;32mIn [1]\u001B[0;36m\u001B[0m\n\u001B[0;31m    import ../m4aim_hd\u001B[0m\n\u001B[0m           ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import ot\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from FrEIA.framework import InputNode, OutputNode, Node, ReversibleGraphNet, ConditionNode\n",
    "from FrEIA.modules import GLOWCouplingBlock\n",
    "\n",
    "batch_size = 1024\n",
    "num_samples_per_epoch = 16384\n",
    "num_epochs_INN = 100 #this is adjusted for runtime\n",
    "DATASET_SIZE = num_samples_per_epoch*num_epochs_INN\n",
    "DIMENSION=100\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the below cell are the helper functions to deal with the toy forward problem $y = AX+ \\eta$. E.g. the DataLoader, forward pass and analytical posteriors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# draw num_samples samples from the distributions given by the mixture_params\n",
    "# returns those samples\n",
    "def draw_mixture_dist(mixture_params, num_samples):\n",
    "    n = len(mixture_params)\n",
    "    sigmas=torch.stack([torch.sqrt(sigma) for w,mu,sigma in mixture_params])\n",
    "    probs=np.array([w for w, mu, sigma in mixture_params])\n",
    "    zs = np.random.choice(n, (num_samples,), p=probs/probs.sum())\n",
    "    mus = torch.stack([mu for w, mu, sigma in mixture_params])[zs]\n",
    "    sigmas_samples = sigmas[zs]\n",
    "    multinomial_samples = torch.randn(num_samples, mus.shape[1], device=device)\n",
    "    if len(sigmas_samples.shape)==1:\n",
    "        sigmas_samples=sigmas_samples.unsqueeze(-1)\n",
    "    out_samples = mus + multinomial_samples*sigmas_samples\n",
    "    return out_samples\n",
    "\n",
    "\n",
    "# gets mean and covariance of the Gaussian posterior with linear forward model\n",
    "# mean, sigma are the parameters of the prior distribution\n",
    "def get_single_gaussian_posterior(mean, sigma, forward_mat, b_sq, y):\n",
    "    ATA = forward_mat**2/b_sq\n",
    "    cov_gauss = 1/(ATA+1/sigma)\n",
    "\n",
    "    mean_gauss = cov_gauss*forward_mat*y/b_sq+cov_gauss*mean/sigma\n",
    "    return mean_gauss, cov_gauss\n",
    "\n",
    "# returns the mixture parameters of the posterior given the mixture parameters of the\n",
    "# prior, the forward model and the likelihood (for a specific y)\n",
    "def get_mixture_posterior(x_gauss_mixture_params, forward_mat, b_sq, y):\n",
    "    out_mixtures = []\n",
    "    nenner = 0\n",
    "    ws=torch.zeros(len(x_gauss_mixture_params),device=device)\n",
    "    mus_new=[]\n",
    "    sigmas_new=[]\n",
    "    log_zaehler=torch.zeros(len(x_gauss_mixture_params),device=device)\n",
    "    for k,(w, mu, sigma) in enumerate(x_gauss_mixture_params):\n",
    "        mu_new, sigma_new = get_single_gaussian_posterior(mu, sigma, forward_mat, b_sq, y)\n",
    "        mus_new.append(mu_new)\n",
    "        sigmas_new.append(sigma_new)\n",
    "        ws[k]=w\n",
    "        log_zaehler[k]=torch.log(torch.tensor(w,device=device,dtype=torch.float))+(0.5*torch.sum(mu_new**2/sigma_new)-0.5*torch.sum(mu**2)/sigma)\n",
    "    const=torch.max(log_zaehler)\n",
    "    log_nenner=torch.log(torch.sum(torch.exp(log_zaehler-const)))+const\n",
    "    for k in range(len(x_gauss_mixture_params)):\n",
    "        out_mixtures.append((torch.exp(log_zaehler[k]-log_nenner).detach().cpu().numpy(),mus_new[k],sigmas_new[k]))\n",
    "    return out_mixtures\n",
    "\n",
    "# creates forward map\n",
    "# scale controls how illposed the problem is\n",
    "\n",
    "def create_forward_model(scale,dimension):\n",
    "    s = torch.ones(dimension, device = device)\n",
    "    for i in range(dimension):\n",
    "        s[i] = scale/(i+1)\n",
    "    return s\n",
    "\n",
    "# evaluates forward_map\n",
    "def forward_pass(x, forward_map):\n",
    "    return x*forward_map\n",
    "\n",
    "\n",
    "# gets the log of the prior of some samples given its mixture parameters\n",
    "\n",
    "def get_prior_log_likelihood(samples, mixture_params):\n",
    "    exponents = torch.zeros((samples.shape[0], len(mixture_params)), device=device)\n",
    "    dimension=samples.shape[1]\n",
    "    for k, (w, mu, sigma) in enumerate(mixture_params):\n",
    "        log_gauss_prefactor = (-dimension / 2)* (np.log(2 * np.pi)   + torch.log(sigma))\n",
    "        tmp = -0.5 * torch.sum((samples - mu[None, :])**2, dim=1)/sigma\n",
    "        exponents[:, k] = tmp + np.log(w) + log_gauss_prefactor\n",
    "\n",
    "    max_exponent = torch.max(exponents, dim=1)[0].detach()\n",
    "    exponents_=exponents-max_exponent.unsqueeze(-1)\n",
    "    exp_sum=torch.log(torch.sum(torch.exp(exponents_),dim=1))+max_exponent\n",
    "    return exp_sum\n",
    "\n",
    "#returns the (negative) log posterior given a y, the mixture params of the prior, the likelihood model b and y\n",
    "def get_log_posterior(samples, forward_map, mixture_params, b, y):\n",
    "    p = -get_prior_log_likelihood(samples, mixture_params)\n",
    "    p2 = 0.5 * torch.sum((y-forward_pass(samples, forward_map))**2 * (1/b**2), dim=1)\n",
    "    return (p+p2).view(len(samples))\n",
    "\n",
    "\n",
    "\n",
    "# creates a data loader returning (x,y) pairs of the joint distribution\n",
    "def get_epoch_data_loader(mixture_params, num_samples_per_epoch, batch_size, forward_map, b):\n",
    "    x = draw_mixture_dist(mixture_params, num_samples_per_epoch)\n",
    "    y = forward_pass(x, forward_map)\n",
    "    y += torch.randn_like(y) * b\n",
    "    def epoch_data_loader():\n",
    "        for i in range(0, num_samples_per_epoch, batch_size):\n",
    "            yield x[i:i+batch_size].clone(), y[i:i+batch_size].clone()\n",
    "\n",
    "    return epoch_data_loader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the below cell the functions to construct the INN are defined, along a custom function to calculate log probabilities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# creates a conditional INN object using the FrEIA package with num_layers many layers,\n",
    "# hidden neurons given by sub_net_size, dimension and dimension_condition specifying the dim of x/y respectively\n",
    "# returns a nn.module object\n",
    "def create_INN(num_layers, sub_net_size,dimension=5,dimension_condition=5):\n",
    "    def subnet_fc(c_in, c_out):\n",
    "        return nn.Sequential(nn.Linear(c_in, sub_net_size), nn.ReLU(),\n",
    "                             nn.Linear(sub_net_size, sub_net_size), nn.ReLU(),\n",
    "                             nn.Linear(sub_net_size,  c_out))\n",
    "    nodes = [InputNode(dimension, name='input')]\n",
    "    cond = ConditionNode(dimension_condition, name='condition')\n",
    "    for k in range(num_layers):\n",
    "        nodes.append(Node(nodes[-1],\n",
    "                          GLOWCouplingBlock,\n",
    "                          {'subnet_constructor':subnet_fc, 'clamp':1.4},\n",
    "                          conditions = cond,\n",
    "                          name=F'coupling_{k}'))\n",
    "    nodes.append(OutputNode(nodes[-1], name='output'))\n",
    "\n",
    "    model = ReversibleGraphNet(nodes + [cond], verbose=False).to(device)\n",
    "    return model\n",
    "\n",
    "# trains an epoch of the INN\n",
    "# given optimizer, the model and the data_loader\n",
    "# training is done via maximum likelihood loss\n",
    "# returns mean loss\n",
    "\n",
    "def train_inn_epoch(optimizer, model, epoch_data_loader, loss_fn, prior_log_pdf, **kwargs):\n",
    "    mean_loss = 0\n",
    "    for k, (x, y) in enumerate(epoch_data_loader()):\n",
    "        cur_batch_size = len(x)\n",
    "\n",
    "        z, jac_inv = model(x, c = y, rev = True)\n",
    "\n",
    "        #l5 = 0.5 * torch.sum(invs**2, dim=1) - jac_inv\n",
    "        #loss += (torch.sum(l5) / cur_batch_size)\n",
    "        loss = loss_fn(z,jac_inv,prior_log_pdf, **kwargs)\n",
    "        print(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        mean_loss = mean_loss * k / (k + 1) + loss.data.item() / (k + 1)\n",
    "    return mean_loss\n",
    "\n",
    "def MLELoss(z,log_det_J,prior_log_pdf, N,prior_params):\n",
    "    M = len(z)\n",
    "    log_p_z = prior_log_pdf(z, prior_params)\n",
    "    return -N/M*log_p_z+N*log_det_J"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some Helper Function for plotting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def make_image(true_samples, pred_samples,inds=None):\n",
    "\n",
    "    cmap = plt.cm.tab20\n",
    "    range_param = 1.2\n",
    "    if inds is None:\n",
    "        no_params = min(5, true_samples.shape[1])\n",
    "        inds=range(no_params)\n",
    "    else:\n",
    "        no_params=len(inds)\n",
    "    fig, axes = plt.subplots(figsize=[12,12], nrows=no_params, ncols=no_params, gridspec_kw={'wspace':0., 'hspace':0.});\n",
    "\n",
    "    for j, ij in enumerate(inds):\n",
    "        for k, ik in enumerate(inds):\n",
    "            axes[j,k].get_xaxis().set_ticks([])\n",
    "            axes[j,k].get_yaxis().set_ticks([])\n",
    "            # if k == 0: axes[j,k].set_ylabel(j)\n",
    "            # if j == len(params)-1: axes[j,k].set_xlabel(k);\n",
    "            if j == k:\n",
    "                axes[j,k].hist(pred_samples[:,ij], bins=50, color=cmap(0), alpha=0.3, range=(-range_param,range_param))\n",
    "                axes[j,k].hist(pred_samples[:,ij], bins=50, color=cmap(0), histtype=\"step\", range=(-range_param,range_param))\n",
    "\n",
    "                axes[j,k].hist(true_samples[:,ij], bins=50, color=cmap(2), alpha=0.3, range=(-range_param,range_param))\n",
    "                axes[j,k].hist(true_samples[:,ij], bins=50, color=cmap(2), histtype=\"step\", range=(-range_param,range_param))\n",
    "            else:\n",
    "                val, x, y = np.histogram2d(pred_samples[:,ij], pred_samples[:,ik], bins=25, range = [[-range_param, range_param], [-range_param, range_param]])\n",
    "                axes[j,k].contour(val, 8, extent=[x[0], x[-1], y[0], y[-1]], alpha=0.5, colors=[cmap(0)])\n",
    "\n",
    "                val, x, y = np.histogram2d(true_samples[:,ij], true_samples[:,ik], bins=25, range = [[-range_param, range_param], [-range_param, range_param]])\n",
    "                axes[j,k].contour(val, 8, extent=[x[0], x[-1], y[0], y[-1]], alpha=0.5, colors=[cmap(2)])\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the cell below the main function to train the INN is defined"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# train from scratch or just use pretrained model\n",
    "retrain=True\n",
    "\n",
    "# trains and evaluates both the INN and SNF and returns the Wasserstein distance on the mixture example\n",
    "# parameters are the mixture params (parameters of the mixture model in the prior), b (likelihood parameter)\n",
    "# a set of testing_ys and the forward model (forward_map)\n",
    "#\n",
    "# prints and returns the Wasserstein distance of INN\n",
    "def train_and_eval(mixture_params, b, testing_ys, forward_map,training_run, loss_fn, prior_log_pdf, **kwargs):\n",
    "\n",
    "    forward_model=lambda x: forward_pass(x, forward_map)\n",
    "    log_posterior=lambda samples,y:get_log_posterior(samples,forward_map,mixture_params,b,y)\n",
    "    INN = create_INN(8,128,dimension=DIMENSION,dimension_condition=DIMENSION)\n",
    "    if retrain:\n",
    "\n",
    "        optimizer_inn = Adam(INN.parameters(), lr = 1e-4)\n",
    "\n",
    "        prog_bar = tqdm(total=num_epochs_INN)\n",
    "\n",
    "        for i in range(num_epochs_INN):\n",
    "            data_loader = get_epoch_data_loader(mixture_params, num_samples_per_epoch, batch_size, forward_map, b)\n",
    "            loss = train_inn_epoch(optimizer_inn, INN, data_loader, loss_fn, prior_log_pdf, **kwargs)\n",
    "            prog_bar.set_description('loss: {:.4f}, b: {}, n_mix: {}'.format(loss, b, len(mixture_params)))\n",
    "            prog_bar.update()\n",
    "        prog_bar.close()\n",
    "\n",
    "        torch.save(INN.state_dict(),'tmp/inn_'+str(training_run)+'.pt')\n",
    "    else:\n",
    "        INN.load_state_dict(torch.load('tmp/inn_'+str(training_run)+'.pt'))\n",
    "\n",
    "    testing_x_per_y = 5000\n",
    "    testing_x_per_y_more = 10000\n",
    "\n",
    "    testing_num_y = len(testing_ys)\n",
    "    weights2 = np.ones((testing_x_per_y,)) / testing_x_per_y\n",
    "\n",
    "    weights2 = weights2.astype(np.float64)\n",
    "    weights2_large = np.ones((testing_x_per_y_more,)) / testing_x_per_y_more\n",
    "\n",
    "    weights2_large = weights2_large.astype(np.float64)\n",
    "    w2=[]\n",
    "\n",
    "\n",
    "    w2_large=[]\n",
    "    tic=time.time()\n",
    "    for i, y in enumerate(testing_ys):\n",
    "        true_posterior_params = get_mixture_posterior(mixture_params, forward_map, b**2, y)\n",
    "        true_posterior_samples = draw_mixture_dist(true_posterior_params, testing_x_per_y).cpu().numpy()\n",
    "        inflated_ys = y[None, :].repeat(testing_x_per_y, 1)\n",
    "        inp_samps=torch.randn(testing_x_per_y, DIMENSION, device=device)\n",
    "        samples_INN = INN(inp_samps, c = inflated_ys)[0].detach().cpu().numpy()\n",
    "\n",
    "        if ((i <10) and (training_run ==0)):\n",
    "            make_image(true_posterior_samples, samples_INN,inds=[0,49,99])\n",
    "\n",
    "        M2 =ot.dist(samples_INN, true_posterior_samples, metric='euclidean')\n",
    "\n",
    "        #w2.append(ot.emd2(weights1, weights2, M2, numItermax=1000000))\n",
    "        #some random operation like taking the mean over all distances to circumvent the weird earth mover distance including the SNF samples\n",
    "        w2.append(np.mean(M2))\n",
    "        true_posterior_samples = draw_mixture_dist(true_posterior_params, testing_x_per_y_more).cpu().numpy()\n",
    "\n",
    "        inflated_ys = y[None, :].repeat(testing_x_per_y_more, 1)\n",
    "        inp_samps=torch.randn(testing_x_per_y_more, DIMENSION, device=device)\n",
    "        samples_INN = INN(inp_samps, c = inflated_ys)[0].detach().cpu().numpy()\n",
    "        M2 =ot.dist(samples_INN, true_posterior_samples, metric='euclidean')\n",
    "\n",
    "        #w2_large.append(ot.emd2(weights1_large, weights2_large, M2, numItermax=1000000))\n",
    "        w2_large.append(np.mean(M2))\n",
    "        toc=time.time()-tic\n",
    "    w2_mean=np.mean(w2)\n",
    "    w2_std=np.std(w2)\n",
    "\n",
    "    w2_mean_large=np.mean(w2_large)\n",
    "    print('W INN:', w2_mean,'+-',w2_std)\n",
    "\n",
    "    print('W INN large:', w2_mean_large)\n",
    "\n",
    "    return w2_mean,w2_mean_large"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the cell below the programm gets executed"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: -400.0757, b: 0.1, n_mix: 12:  65%|██████▌   | 13096/20000 [35:59<16:05,  7.15it/s]\u001B[A\n",
      "loss: -400.0757, b: 0.1, n_mix: 12:  65%|██████▌   | 13097/20000 [35:59<16:02,  7.17it/s]\u001B[A\n",
      "loss: -400.3106, b: 0.1, n_mix: 12:  65%|██████▌   | 13097/20000 [35:59<16:02,  7.17it/s]\u001B[A\n",
      "loss: -400.3106, b: 0.1, n_mix: 12:  65%|██████▌   | 13098/20000 [35:59<15:41,  7.33it/s]\u001B[A\n",
      "loss: -400.3415, b: 0.1, n_mix: 12:  65%|██████▌   | 13098/20000 [35:59<15:41,  7.33it/s]\u001B[A\n",
      "loss: -400.3415, b: 0.1, n_mix: 12:  65%|██████▌   | 13099/20000 [35:59<15:25,  7.46it/s]\u001B[A"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([30928938., 32199748., 31460032.,  ..., 28569314., 33769996.,\n",
      "        30925250.], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 12>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     23\u001B[0m testing_xs \u001B[38;5;241m=\u001B[39m draw_mixture_dist(mixture_params, testing_num_y)\n\u001B[1;32m     24\u001B[0m testing_ys \u001B[38;5;241m=\u001B[39m forward_pass(testing_xs, forward_map) \u001B[38;5;241m+\u001B[39m b \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(testing_num_y, DIMENSION, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m---> 26\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_and_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmixture_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtesting_ys\u001B[49m\u001B[43m,\u001B[49m\u001B[43mforward_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtraining_run\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mr\u001B[49m\u001B[43m,\u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mMLELoss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprior_log_pdf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mget_prior_log_likelihood\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mN\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mDATASET_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprior_params\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmixture_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m results_array[r,\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m results[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     28\u001B[0m results_array[r,\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m results[\u001B[38;5;241m1\u001B[39m]\n",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36mtrain_and_eval\u001B[0;34m(mixture_params, b, testing_ys, forward_map, training_run, loss_fn, prior_log_pdf, **kwargs)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs_INN):\n\u001B[1;32m     21\u001B[0m     data_loader \u001B[38;5;241m=\u001B[39m get_epoch_data_loader(mixture_params, num_samples_per_epoch, batch_size, forward_map, b)\n\u001B[0;32m---> 22\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_inn_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer_inn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mINN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprior_log_pdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m     prog_bar\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss: \u001B[39m\u001B[38;5;132;01m{:.4f}\u001B[39;00m\u001B[38;5;124m, b: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, n_mix: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(loss, b, \u001B[38;5;28mlen\u001B[39m(mixture_params)))\n\u001B[1;32m     24\u001B[0m     prog_bar\u001B[38;5;241m.\u001B[39mupdate()\n",
      "Input \u001B[0;32mIn [3]\u001B[0m, in \u001B[0;36mtrain_inn_epoch\u001B[0;34m(optimizer, model, epoch_data_loader, loss_fn, prior_log_pdf, **kwargs)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28mprint\u001B[39m(loss)\n\u001B[1;32m     38\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 39\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     43\u001B[0m mean_loss \u001B[38;5;241m=\u001B[39m mean_loss \u001B[38;5;241m*\u001B[39m k \u001B[38;5;241m/\u001B[39m (k \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m+\u001B[39m loss\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m/\u001B[39m (k \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/environments/master/lib/python3.8/site-packages/torch/_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/environments/master/lib/python3.8/site-packages/torch/autograd/__init__.py:166\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    162\u001B[0m inputs \u001B[38;5;241m=\u001B[39m (inputs,) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs, torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;28;01melse\u001B[39;00m \\\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;28mtuple\u001B[39m(inputs) \u001B[38;5;28;01mif\u001B[39;00m inputs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m()\n\u001B[1;32m    165\u001B[0m grad_tensors_ \u001B[38;5;241m=\u001B[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001B[38;5;28mlen\u001B[39m(tensors))\n\u001B[0;32m--> 166\u001B[0m grad_tensors_ \u001B[38;5;241m=\u001B[39m \u001B[43m_make_grads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_grads_batched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retain_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n",
      "File \u001B[0;32m~/environments/master/lib/python3.8/site-packages/torch/autograd/__init__.py:67\u001B[0m, in \u001B[0;36m_make_grads\u001B[0;34m(outputs, grads, is_grads_batched)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out\u001B[38;5;241m.\u001B[39mrequires_grad:\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m out\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 67\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad can be implicitly created only for scalar outputs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     68\u001B[0m     new_grads\u001B[38;5;241m.\u001B[39mappend(torch\u001B[38;5;241m.\u001B[39mones_like(out, memory_format\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mpreserve_format))\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "#numbers of testing_ys\n",
    "testing_num_y = 100\n",
    "# likelihood parameter\n",
    "b = 0.1\n",
    "# forward_model\n",
    "forward_map = create_forward_model(scale = 0.1,dimension=DIMENSION)\n",
    "# number of mixtures\n",
    "n_mixtures=12\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "results_array = np.zeros((5,2))\n",
    "for r in range(5):\n",
    "\n",
    "    mixture_params=[]\n",
    "    # create mixture params (weights, means, covariances)\n",
    "    for i in range(n_mixtures):\n",
    "        mixture_params.append((1./n_mixtures,torch.tensor(np.random.uniform(size=DIMENSION)*2-1, device = device,dtype=torch.float),torch.tensor(0.0001,device=device,dtype=torch.float)))\n",
    "    if not os.path.exists('tmp'):\n",
    "        os.makedirs('tmp')\n",
    "    with open('tmp/models_mixture' +str(r), 'wb') as fp:\n",
    "        pickle.dump(mixture_params, fp)\n",
    "    # draws testing_ys\n",
    "    testing_xs = draw_mixture_dist(mixture_params, testing_num_y)\n",
    "    testing_ys = forward_pass(testing_xs, forward_map) + b * torch.randn(testing_num_y, DIMENSION, device=device)\n",
    "\n",
    "    results = train_and_eval(mixture_params,b,testing_ys,forward_map,training_run = r,loss_fn = MLELoss, prior_log_pdf = get_prior_log_likelihood, N = DATASET_SIZE, prior_params = mixture_params)\n",
    "    results_array[r,0] = results[0]\n",
    "    results_array[r,1] = results[1]\n",
    "\n",
    "\n",
    "print('MEANS OF INN')\n",
    "print(np.mean(results_array[:,0]))\n",
    "\n",
    "print('STD OF INN')\n",
    "print(np.std(results_array[:,0]))\n",
    "\n",
    "print('MEAN OF INN LARGE')\n",
    "print(np.mean(results_array[:,1]))\n",
    "\n",
    "print('STD OF INN LARGE')\n",
    "print(np.std(results_array[:,1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'm4aim_hd'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mm4aim_hd\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mhaemod\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'm4aim_hd'"
     ]
    }
   ],
   "source": [
    "import m4aim_hd.src as haemod\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
